{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speculative decoding demo\n",
        "\n",
        "In this quick demo we will implement and run speculative decoding and compare it with classic autoregressive generation"
      ],
      "metadata": {
        "id": "SBsoCprbUeoj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed8QNvEjUb3k"
      },
      "outputs": [],
      "source": [
        "# Prepare environment\n",
        "!pip install transformers>4.51.0 torch optimum-quanto"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoConfig\n",
        "from transformers.cache_utils import DynamicCache\n",
        "import time\n",
        "import torch"
      ],
      "metadata": {
        "id": "WiDluZIlU-6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use <code>Qwen/Qwen3-4B</code> as target model and <code>Qwen/Qwen3-1.7B</code> as draft model.\n",
        "\n",
        "To make sure we can run inference on the 16GB T4 GPU Colab provides, we quantize the model parameters at runtime at FP8.\n",
        "\n",
        "Note that we cannot use the pre-quantized versions of these models available on HuggingFace as they require compute capability > 8.9 (NVIDIA Hopper architecture)."
      ],
      "metadata": {
        "id": "U-0zcYWNVCrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading models from HuggingFace will take around 3-4 minutes"
      ],
      "metadata": {
        "id": "oaHo6sKeXuCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using regular Qwen models with runtime quantization for low memory footprint\n",
        "target_model_name = \"Qwen/Qwen3-4B\"\n",
        "draft_model_name = \"Qwen/Qwen3-1.7B\"\n",
        "\n",
        "target_quantize = QuantoConfig(weights=\"float8\")\n",
        "draft_quantize = QuantoConfig(weights=\"float8\")\n",
        "\n",
        "# Initialize target and draft models with FP8 quantization\n",
        "target = AutoModelForCausalLM.from_pretrained(\n",
        "    target_model_name,\n",
        "    quantization_config=target_quantize,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "draft = AutoModelForCausalLM.from_pretrained(\n",
        "    draft_model_name,\n",
        "    quantization_config=draft_quantize,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
        "\n",
        "# Move models to device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "target.to(device)\n",
        "draft.to(device)"
      ],
      "metadata": {
        "id": "uy2Gcd-DVBSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we prepare the input prompt"
      ],
      "metadata": {
        "id": "wLOias9MVoJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input\n",
        "prompt = \"What is the result of 2 * pi?\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        "    enable_thinking=False\n",
        ")\n",
        "input_ids = tokenizer([text], return_tensors=\"pt\").to(target.device)  # Tokenized input for models\n",
        "\n",
        "print(f\"Prepared prompt: \\n\\n{text}\")"
      ],
      "metadata": {
        "id": "YJcKlVwyVmRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoregressive generation with Target model"
      ],
      "metadata": {
        "id": "gW6MhAVbV1D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_output_length = 256  # Includes prompt as well\n",
        "output_ids = target.generate(**input_ids, max_new_tokens=max_output_length)\n",
        "\n",
        "content = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True).strip('\\n')\n",
        "\n",
        "print(f\"Target model Autoregressive generation output: \\n\\n{content}\")"
      ],
      "metadata": {
        "id": "MaOnkHbcV4YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Speculative decoding generation"
      ],
      "metadata": {
        "id": "B7qyPtMBWBKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define helper functions"
      ],
      "metadata": {
        "id": "3-gUjFxWWD0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decoder(logits: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "def top_k_decoder(logits: torch.Tensor, _k: int) -> torch.Tensor:\n",
        "    return torch.topk(logits, _k, dim=-1).indices.unsqueeze(-1)\n",
        "\n",
        "def prune_cache(cache: DynamicCache, num_tokens_to_discard: int) -> DynamicCache:\n",
        "    if cache is None:\n",
        "        return None\n",
        "\n",
        "    current_length = cache.get_seq_length()\n",
        "    new_length = current_length - num_tokens_to_discard\n",
        "\n",
        "    cache.crop(new_length)\n",
        "\n",
        "    return cache"
      ],
      "metadata": {
        "id": "_I93WQPcWHth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speculative decoding implementation"
      ],
      "metadata": {
        "id": "HmP1U0ufWNsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def speculative_generate(\n",
        "    target,\n",
        "    draft,\n",
        "    tokenizer,\n",
        "    decoder,\n",
        "    input_ids,\n",
        "    max_output_length,\n",
        "    gamma,\n",
        "    k\n",
        "):\n",
        "    current_position = len(input_ids[0])\n",
        "    input_ids_sd = torch.full((1, max_output_length), tokenizer.pad_token_id, dtype=torch.long, device=target.device)\n",
        "    input_ids_sd[0, :current_position] = input_ids[0].detach().clone().to(target.device)\n",
        "\n",
        "    target_cache, draft_cache = None, None\n",
        "\n",
        "    # Prefill target model to get a first token and cache\n",
        "    target_output = target(\n",
        "        input_ids=input_ids_sd[..., :current_position],\n",
        "        past_key_values=target_cache,\n",
        "        use_cache=False\n",
        "    )\n",
        "    target_cache = target_output.past_key_values  # Set KV Cache for target model\n",
        "    token = decoder(target_output.logits[..., -1, :])\n",
        "    input_ids_sd[0, current_position] = token\n",
        "    current_position += 1\n",
        "\n",
        "    if token == tokenizer.eos_token_id:\n",
        "        return input_ids_sd[0, :current_position].tolist()\n",
        "\n",
        "    while current_position < max_output_length:\n",
        "        corrected_gamma = min(gamma, max_output_length - current_position - 1)\n",
        "\n",
        "        # Generate gamma drafty tokens\n",
        "        for k in range(corrected_gamma):\n",
        "            draft_output = draft(\n",
        "                input_ids=input_ids_sd[..., :current_position + k],\n",
        "                past_key_values=draft_cache,\n",
        "                use_cache=False\n",
        "            )\n",
        "            draft_cache = draft_output.past_key_values\n",
        "            token = decoder(draft_output.logits[..., -1, :])\n",
        "            input_ids_sd[0, current_position + k] = token\n",
        "\n",
        "        # Validate drafty tokens in parallel\n",
        "        target_output = target(\n",
        "            input_ids=input_ids_sd[..., :current_position + corrected_gamma],\n",
        "            past_key_values=target_cache,\n",
        "            use_cache=False\n",
        "        )\n",
        "        target_cache = target_output.past_key_values\n",
        "        target_logits = target_output.logits[..., current_position - 1: current_position + corrected_gamma - 1, :]\n",
        "        target_topk = top_k_decoder(target_logits, k)\n",
        "\n",
        "        # Compute the last accepted draft position\n",
        "        n = corrected_gamma\n",
        "        for i in range(corrected_gamma):\n",
        "            if input_ids_sd[0, current_position + i] not in target_topk[0, i, :]:\n",
        "                n = i\n",
        "                break\n",
        "\n",
        "        # Check if any of the accepted tokens is the eos token\n",
        "        stop_locations = torch.nonzero(torch.eq(input_ids_sd[0, current_position: current_position + n], tokenizer.eos_token_id))\n",
        "        if stop_locations.shape[0] > 0:\n",
        "            stop_location = stop_locations[0, 0].item()\n",
        "            input_ids_sd[0, current_position + stop_location + 1:] = tokenizer.pad_token_id\n",
        "            current_position += stop_location + 1\n",
        "            break\n",
        "\n",
        "        if n != corrected_gamma:\n",
        "            # Need to adjust the cache of both models\n",
        "            draft_cache = prune_cache(draft_cache, corrected_gamma - n)\n",
        "            target_cache = prune_cache(target_cache, corrected_gamma - n + 1)\n",
        "\n",
        "        # Next token is sampled from the target model's distribution\n",
        "        target_logits = target_output.logits[..., current_position + n - 1, :]\n",
        "        token = decoder(target_logits)\n",
        "\n",
        "        input_ids_sd[0, current_position + n: current_position + corrected_gamma] = tokenizer.pad_token_id\n",
        "        input_ids_sd[0, current_position + n] = token\n",
        "        current_position += n + 1\n",
        "\n",
        "        if token == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return input_ids_sd[0, :current_position].tolist()"
      ],
      "metadata": {
        "id": "VWMOb1oTWJte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 5\n",
        "k = 10  # Accept drafty tokens only if they are in the top k of the target model\n",
        "max_output_length = 256\n",
        "\n",
        "output_ids = speculative_generate(\n",
        "    target,\n",
        "    draft,\n",
        "    tokenizer,\n",
        "    greedy_decoder,\n",
        "    input_ids['input_ids'],\n",
        "    max_output_length,\n",
        "    gamma,\n",
        "    k\n",
        ")\n",
        "\n",
        "content = tokenizer.decode(output_ids, skip_special_tokens=True).strip('\\n')\n",
        "\n",
        "print(f\"Speculative decoding output: \\n\\n{content}\")"
      ],
      "metadata": {
        "id": "6Xa4s6R1WQfL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}