{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Programming: Perform vector and matrix operations in CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Setup\n",
        "\n",
        "You can run this notebook on either Colab or clone the github repo to your virtual machine with GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### This script is used to clone the repository to the google drive and setup the directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive_path = '/content/drive' \n",
        "drive.mount(drive_path)\n",
        "workspace_path = os.path.join(drive_path, \"MyDrive/workspace/11868hw\")\n",
        "!mkdir -p {workspace_path}\n",
        "%cd {workspace_path}\n",
        "git_repo_name = \"llmsys_code_examples\"\n",
        "repo_path = os.path.join(workspace_path, git_repo_name)\n",
        "if os.path.isdir(os.path.join(repo_path, \".git\")):\n",
        "  %cd {git_repo_name}\n",
        "  !git pull\n",
        "else: \n",
        "  !git clone https://github.com/llmsystem/llmsys_code_examples.git\n",
        "%cd {repo_path}/simple_cuda_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### This script is used to install cuda 12.4\n",
        "\n",
        "!apt-get remove --purge cuda-* nvidia-* -y\n",
        "!apt-get autoremove -y\n",
        "!apt-get clean\n",
        "!rm -rf /usr/local/cuda*\n",
        "\n",
        "# Install 12.4\n",
        "!apt-get install -y cuda-toolkit-12-4\n",
        "\n",
        "# Check versions\n",
        "!ls /usr/local | grep cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section1: Compile and Run CUDA Code\n",
        "\n",
        "We will implment a program to add two vectors in cuda. \n",
        "The following code snippt is from `example_vector_add.cu`. \n",
        "\n",
        "```cpp\n",
        "\n",
        "__global__ void VecAddKernel(int* A, int* B, int* C, int n) {\n",
        "  // blockDim is size of block along x-axis\n",
        "  // blockIdx is the index of the current thread's block\n",
        "  // threadIdx is the index of the current thread within the block\n",
        "  int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "  if (i < n) {\n",
        "    C[i] = A[i] + B[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "Please check the full code in `example_vector_add.cu`. \n",
        "\n",
        "Run the following command: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove ! if running on the terminal of your vm\n",
        "# Compile the codes for matrix addition\n",
        "!nvcc -o vecadd example_vector_add.cu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following command to check the result: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!./vecadd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In `example_matadd.cu` and `example_matmul2.cu`, there are example codes for matrix addition and matrix multiplication. \n",
        "\n",
        "The following code snippets are from `example_matadd.cu` to perform matrix multiplication:\n",
        "\n",
        "```cpp\n",
        "\n",
        "__global__ void matrixAdd(const int * a, const int * b,\n",
        "                          int * c, int N) {\n",
        "  // Compute each thread's global row and column index\n",
        "  int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  \n",
        "  // Iterate over row, and down column\n",
        "  if (row < N && col < N) {\n",
        "    c[row * N + col] = a[row * N + col] + b[row * N + col];\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "Run the following command to check the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove ! if running on the terminal of your vm\n",
        "# Compile the codes for matrix addition\n",
        "!nvcc -o matadd example_matadd.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the codes\n",
        "!./matadd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code snippets illustrate the multiplication of two matrices. \n",
        "\n",
        "```c++\n",
        "\n",
        "__global__ void MatmulKernel(const float* a, const float* b, float* out, \n",
        "                             int M, int N, int P) {\n",
        "  // Compute each thread's global row and column index\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if (idx >= M * P) return;\n",
        "  int row = idx / P;\n",
        "  int col = idx % P;\n",
        "  if (row < M && col < P) {\n",
        "    // Calculate the matrix multiplication for row in matrix a and col in matrix b\n",
        "    float sum = 0.0;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "      sum += a[row * N + i] * b[i * P + col];\n",
        "    }\n",
        "    out[row * P + col] = sum;\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "```c++\n",
        "__global__ void MatmulKernel(const int *a, const int *b, int *c, int M, int N, int P) {\n",
        "  // Compute each thread's global row and column index\n",
        "  int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  if (row >= M || col >= P) return;\n",
        "  // Iterate over row, and down column\n",
        "  c[row * P + col] = 0;\n",
        "  for (int k = 0; k < N; k++) {\n",
        "    // Accumulate results for a single element\n",
        "    c[row * P + col] += a[row * N + k] * b[k * P + col];\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following command. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the codes for matrix addition\n",
        "!nvcc -o matmul example_matmul2.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the codes\n",
        "!./matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section2: Run CUDA Codes with Python Calls\n",
        "\n",
        "This section shows three demos, including the vector addition, window summation and matrix multiplication implemented in CUDA. We call these CUDA functions within the python codes, which follows the same recipe in our assignment1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m1.5/1.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2023.1.1-py2.py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.3)\n",
            "Building wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661205 sha256=87b18480433e8cc46e8387b1463bb1b1fa14bd3c2f1dcf7f9a33451b88e59872\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.0 pycuda-2024.1 pytools-2023.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUDA Example for Vector Add\n",
        "\n",
        "We demonstrate how we can call C function in python file with ctypes with `VecAddCPU` function.\n",
        "\n",
        "We also demonstrate two ways to write CUDA codes which can be called in python functions. The difference between them is that we create CUDA memory and copy the data to CUDA device by `pycuda` package in the `VecAddCUDA` function and `cudaMemcpy` in cpp codes in `VecAddCUDA2` function.\n",
        "\n",
        "We use `pycuda` in assignment1 to call CUDA kernel functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc -o vector_add.so --shared example_vector_add.cu -Xcompiler -fPIC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "# Define the argument types and return types\n",
        "lib.VecAddCUDA.argtypes = [\n",
        "    ctypes.POINTER(ctypes.c_int),\n",
        "    ctypes.POINTER(ctypes.c_int),\n",
        "    ctypes.POINTER(ctypes.c_int),\n",
        "    ctypes.c_int,\n",
        "]\n",
        "\n",
        "lib.VecAddCUDA.restype = None\n",
        "\n",
        "# Load the arrays to CUDA device\n",
        "a_gpu = gpuarray.to_gpu(a)\n",
        "b_gpu = gpuarray.to_gpu(b)\n",
        "c_gpu = gpuarray.to_gpu(cgpu)\n",
        "\n",
        "# Call the C wrapper function with CUDA kernel\n",
        "lib.VecAddCUDA(\n",
        "    ctypes.cast(a_gpu.ptr, ctypes.POINTER(ctypes.c_int)),\n",
        "    ctypes.cast(b_gpu.ptr, ctypes.POINTER(ctypes.c_int)),\n",
        "    ctypes.cast(c_gpu.ptr, ctypes.POINTER(ctypes.c_int)),\n",
        "    ctypes.c_int(size)\n",
        ")\n",
        "\n",
        "# Load the gpuarray back to array in the host device\n",
        "cgpu = c_gpu.get()\n",
        "print(f\"After offload: {cgpu}, {type(cgpu)}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input a: [7 5 1 7 6 4 7 3 9 3]\n",
            "Input b: [8 4 2 6 5 1 7 7 8 6]\n",
            "Numpy add: [15  9  3 13 11  5 14 10 17  9], <class 'numpy.ndarray'>\n",
            "CPU add: [15  9  3 13 11  5 14 10 17  9], <class 'numpy.ndarray'>\n",
            "GPU add: [15  9  3 13 11  5 14 10 17  9], <class 'pycuda.gpuarray.GPUArray'>\n",
            "After offload: [15  9  3 13 11  5 14 10 17  9], <class 'numpy.ndarray'>\n",
            "GPU add2: [15  9  3 13 11  5 14 10 17  9], <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "!python test_vector_add.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUDA Example for Window Sum\n",
        "\n",
        "Demo of Window Sum to get to know about synchronization in CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc -o window_sum.so --shared example_window_sum.cu -Xcompiler -fPIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n",
            "Numpy window sum: [15. 20. 25. 30. 35. 40. 45. 50.]\n",
            "GPU simple window sum: [15. 20. 25. 30. 35. 40. 45. 50.]\n",
            "GPU shared window sum: [15. 20. 25. 30. 35. 40. 45. 50.]\n"
          ]
        }
      ],
      "source": [
        "!python test_window_sum.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUDA Example for Matrix Multiplication\n",
        "\n",
        "Demo of matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc -o matmul.so --shared example_matmul.cu -Xcompiler -fPIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input a: [[1. 2. 2. 1.]\n",
            " [2. 2. 2. 2.]\n",
            " [2. 2. 2. 1.]\n",
            " [1. 2. 1. 1.]]\n",
            "Input b: [[1. 1.]\n",
            " [1. 2.]\n",
            " [1. 2.]\n",
            " [2. 1.]]\n",
            "Numpy matmul: [[ 7. 10.]\n",
            " [10. 12.]\n",
            " [ 8. 11.]\n",
            " [ 6.  8.]], <class 'numpy.ndarray'>\n",
            "GPU matmul: [[ 7. 10.]\n",
            " [10. 12.]\n",
            " [ 8. 11.]\n",
            " [ 6.  8.]], <class 'pycuda.gpuarray.GPUArray'>\n",
            "After offload: [[ 7. 10.]\n",
            " [10. 12.]\n",
            " [ 8. 11.]\n",
            " [ 6.  8.]], <class 'numpy.ndarray'>\n",
            "Compare result: 0.0\n"
          ]
        }
      ],
      "source": [
        "!python test_matmul.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
