{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import defaultdict \n",
    "import string\n",
    "\n",
    "SPACE_MARK = \"_\"\n",
    "\n",
    "def get_init_vocab(data, use_sentence_piece=False): \n",
    "    \"\"\" \n",
    "    Given a list of strings, returns a dictionary of words mapping to their frequency  \n",
    "    count in the data. \n",
    "    Args: \n",
    "        data: raw text with line breaks\n",
    "        \n",
    "    Returns: \n",
    "        (vocab, tokens) tuple, \n",
    "          vocab is a dictionary mapping space delimited characters to count (e.g. {'a b c </w>': 5})\n",
    "          tokens is a set of basic characters. \n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    tokens = set()\n",
    "    tokens.add('</w>')\n",
    "    for line in data: \n",
    "        for word in line.split(): \n",
    "            if use_sentence_piece:\n",
    "                word = '_' + word\n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "            tokens.update(list(word))\n",
    "    return vocab, tokens \n",
    "  \n",
    "def count_cooccurance(vocab): \n",
    "    \"\"\" \n",
    "    Given a vocabulary (dictionary mapping words to frequency counts), returns a  \n",
    "    dictionary of tuples representing the frequency count of pairs of characters  \n",
    "    in the vocabulary. \n",
    "    Args:\n",
    "        vocab: a dictionary mapping space-delimited tokens to count (e.g. {'a b c </w>': 5})\n",
    "        \n",
    "    Returns: \n",
    "        a dictionary mapping a tuple of tokens to count\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int) \n",
    "    for word, freq in vocab.items(): \n",
    "        chars = word.split() # split the word by any white space\n",
    "        for i in range(len(chars)-1): \n",
    "            pairs[chars[i], chars[i+1]] += freq \n",
    "    return pairs\n",
    "  \n",
    "def merge_vocab(token_pair, vocab_in): \n",
    "    \"\"\" \n",
    "    Given a pair of tokens and a vocabulary, returns a new vocabulary with the  \n",
    "    pair of tokens merged together wherever they appear. \n",
    "    \n",
    "    e.g. merge_vocab(('a', 'b'), {'a b c </w>': 5})\n",
    "    returns {'ab c </w>': 5}\n",
    "    \n",
    "    Args: \n",
    "        token_pair: a tuple of two tokens\n",
    "        vocab_in: a dictionary mapping space-delimited tokens to count (e.g. {'a b c </w>': 5})\n",
    "        \n",
    "    Returns: \n",
    "        a dictionary mapping space-delimited tokens to count (e.g. {'a b c </w>': 5})\n",
    "    \"\"\"\n",
    "    vocab_out = defaultdict(int)  \n",
    "    bigram = re.escape(' '.join(token_pair)) \n",
    "    new_token = ''.join(token_pair)\n",
    "    # search for every occurance of bigram (token pairs with a space), \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "    for word in vocab_in:\n",
    "        # replace the bigram (with space), with the new merged token (the concanated pair)\n",
    "        w_out = p.sub(new_token, word)\n",
    "        vocab_out[w_out] = vocab_in[word]\n",
    "    return vocab_out\n",
    "\n",
    "  \n",
    "def byte_pair_encoding(data, n, use_sentence_piece=False): \n",
    "    \"\"\" \n",
    "    Given a list of strings and an integer n, returns a list of n merged pairs \n",
    "    of characters found in the vocabulary of the input data. \n",
    "    \n",
    "    Args: \n",
    "        data: raw text\n",
    "        n: number of merge opperations\n",
    "    \n",
    "    Returns: \n",
    "        a list of tokens\n",
    "        a dictionary mapping token to index (starting from 0)\n",
    "    \"\"\"\n",
    "    vocab, init_tokens = get_init_vocab(data, use_sentence_piece=use_sentence_piece)\n",
    "    tokens = list(init_tokens)\n",
    "    for i in range(n): \n",
    "        pairs = count_cooccurance(vocab) \n",
    "        best_pair = max(pairs, key=pairs.get) \n",
    "        new_token = ''.join(best_pair)\n",
    "        tokens.append(new_token)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        print('step {}: merging \\\"{}\\\" and \\\"{}\\\"'.format(i+1, best_pair[0], best_pair[1]))\n",
    "    token_to_ids = dict([(tk, id) for id, tk in enumerate(tokens)])\n",
    "    return tokens, token_to_ids\n",
    "\n",
    "def tokenize(data, token_dict):\n",
    "    \"\"\"\n",
    "    split the data into tokens and map into index. \n",
    "    It applies greedy split to text with longest matching.\n",
    "    \n",
    "    e.g. \n",
    "    tokenize(\"spiderman\", {'spider':0,'man': 1})\n",
    "    will return\n",
    "     [0, 1]\n",
    "        \n",
    "    Args: \n",
    "        data: raw text\n",
    "        token_dict: a dictionary mapping from token to id\n",
    "        \n",
    "    Returns: \n",
    "        a list of ids\n",
    "        \n",
    "    \"\"\"\n",
    "    encoded_ids = []\n",
    "    for line in data: \n",
    "        for word in line.split():\n",
    "            word = word + '</w>'\n",
    "            last_idx = 0\n",
    "            idx = len(word)\n",
    "            while idx > last_idx:\n",
    "                whole_word = word[last_idx:idx]\n",
    "                if whole_word in token_dict:\n",
    "                    # Match the prefix\n",
    "                    encoded_ids.append(token_dict[whole_word])\n",
    "                    last_idx = idx\n",
    "                    idx = len(word)\n",
    "                else:\n",
    "                    idx = idx - 1\n",
    "    return encoded_ids\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: merging \"e\" and \"r\"\n",
      "step 2: merging \"s\" and \"</w>\"\n",
      "step 3: merging \"e\" and \"</w>\"\n",
      "step 4: merging \"e\" and \"n\"\n",
      "step 5: merging \"d\" and \"</w>\"\n",
      "step 6: merging \"h\" and \"er\"\n",
      "step 7: merging \"en\" and \"t\"\n",
      "step 8: merging \"e\" and \"d</w>\"\n",
      "step 9: merging \",\" and \"</w>\"\n",
      "step 10: merging \"her\" and \"</w>\"\n",
      "step 11: merging \"n\" and \"</w>\"\n",
      "step 12: merging \"p\" and \"a\"\n",
      "step 13: merging \"pa\" and \"r\"\n",
      "step 14: merging \"par\" and \"ent\"\n",
      "step 15: merging \"en\" and \"</w>\"\n",
      "step 16: merging \"h\" and \"e</w>\"\n",
      "step 17: merging \"a\" and \"s</w>\"\n",
      "step 18: merging \"s\" and \"e\"\n",
      "step 19: merging \"e\" and \"a\"\n",
      "step 20: merging \"i\" and \"t\"\n",
      "The bpe tokens are: \n",
      "w: 0\n",
      "D: 1\n",
      "k: 2\n",
      "T: 3\n",
      "h: 4\n",
      "n: 5\n",
      "p: 6\n",
      "o: 7\n",
      "]: 8\n",
      "b: 9\n",
      "m: 10\n",
      "u: 11\n",
      "a: 12\n",
      "W: 13\n",
      "r: 14\n",
      "B: 15\n",
      "v: 16\n",
      "e: 17\n",
      "t: 18\n",
      "6: 19\n",
      "[: 20\n",
      "l: 21\n",
      "d: 22\n",
      "y: 23\n",
      ",: 24\n",
      "c: 25\n",
      "f: 26\n",
      "O: 27\n",
      "g: 28\n",
      "H: 29\n",
      "</w>: 30\n",
      "s: 31\n",
      "': 32\n",
      "i: 33\n",
      "er: 34\n",
      "s</w>: 35\n",
      "e</w>: 36\n",
      "en: 37\n",
      "d</w>: 38\n",
      "her: 39\n",
      "ent: 40\n",
      "ed</w>: 41\n",
      ",</w>: 42\n",
      "her</w>: 43\n",
      "n</w>: 44\n",
      "pa: 45\n",
      "par: 46\n",
      "parent: 47\n",
      "en</w>: 48\n",
      "he</w>: 49\n",
      "as</w>: 50\n",
      "se: 51\n",
      "ea: 52\n",
      "it: 53\n",
      "The ids of the tokenized sequence are: \n",
      "[15, 34, 10, 12, 5, 32, 35, 47, 35, 22, 33, 16, 7, 14, 25, 41, 0, 4, 48, 49, 0, 50, 51, 16, 48, 3, 39, 52, 26, 18, 34, 42, 49, 31, 6, 21, 53, 30, 18, 33, 10, 36, 9, 17, 18, 0, 17, 48, 52, 25, 4, 30, 47, 32, 35, 4, 7, 11, 51, 4, 7, 21, 38, 11, 5, 18, 33, 21, 30, 49, 40, 34, 41, 25, 7, 21, 21, 17, 28, 36, 20, 19, 8, 30, 29, 33, 35, 26, 12, 18, 43, 14, 17, 21, 7, 25, 12, 18, 41, 18, 7, 30, 1, 12, 21, 21, 50, 26, 7, 14, 30, 12, 30, 6, 7, 31, 53, 33, 7, 44, 50, 12, 30, 21, 7, 9, 9, 23, 33, 31, 18, 30, 7, 44, 9, 17, 4, 12, 21, 26, 30, 7, 26, 30, 26, 7, 7, 22, 51, 14, 16, 33, 25, 36, 9, 11, 31, 33, 5, 17, 31, 51, 31, 42, 0, 4, 33, 21, 36, 4, 33, 35, 10, 7, 18, 43, 10, 7, 16, 41, 9, 12, 25, 2, 30, 33, 44, 0, 53, 4, 30, 43, 47, 35, 33, 44, 13, 7, 7, 31, 18, 34, 42, 27, 4, 33, 7, 42, 12, 5, 38, 9, 17, 25, 12, 10, 36, 12, 30, 18, 52, 25, 43, 18, 39, 36]\n",
      "\n",
      "The sequence corresponding to ids is: \n",
      "B er m a n ' s</w> parent s</w> d i v o r c ed</w> w h en</w> he</w> w as</w> se v en</w> T her ea f t er ,</w> he</w> s p l it </w> t i m e</w> b e t w e en</w> ea c h </w> parent ' s</w> h o u se h o l d</w> u n t i l </w> he</w> ent er ed</w> c o l l e g e</w> [ 6 ] </w> H i s</w> f a t her</w> r e l o c a t ed</w> t o </w> D a l l as</w> f o r </w> a </w> p o s it i o n</w> as</w> a </w> l o b b y i s t </w> o n</w> b e h a l f </w> o f </w> f o o d se r v i c e</w> b u s i n e s se s ,</w> w h i l e</w> h i s</w> m o t her</w> m o v ed</w> b a c k </w> i n</w> w it h </w> her</w> parent s</w> i n</w> W o o s t er ,</w> O h i o ,</w> a n d</w> b e c a m e</w> a </w> t ea c her</w> t her e</w>\n"
     ]
    }
   ],
   "source": [
    "# Example usage: \n",
    "corpus = '''Berman's parents divorced when he was seven. \n",
    "Thereafter, he split time between each parent's household until he entered college.[6] \n",
    "His father relocated to Dallas for a position as a lobbyist on behalf of foodservice businesses, \n",
    "while his mother moved back in with her parents in Wooster, Ohio, and became a teacher there'''\n",
    "data = corpus.split('.') \n",
    "  \n",
    "n = 20 # number of merge operations\n",
    "id_to_tokens, token_to_ids = byte_pair_encoding(data, n)\n",
    "\n",
    "token_ids = tokenize(data, token_to_ids)\n",
    "\n",
    "print(\"The bpe tokens are: \")\n",
    "for tk, tid in token_to_ids.items():\n",
    "    print(\"{}: {}\".format(tk, tid))\n",
    "\n",
    "print(\"The ids of the tokenized sequence are: \")\n",
    "print(token_ids)\n",
    "print()\n",
    "print(\"The sequence corresponding to ids is: \")\n",
    "print(' '.join(id_to_tokens[tid] for tid in token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: merging \"e\" and \"r\"\n",
      "step 2: merging \"s\" and \"</w>\"\n",
      "step 3: merging \"e\" and \"</w>\"\n",
      "step 4: merging \"e\" and \"n\"\n",
      "step 5: merging \"d\" and \"</w>\"\n",
      "step 6: merging \"_\" and \"h\"\n",
      "step 7: merging \"h\" and \"er\"\n",
      "step 8: merging \"_\" and \"b\"\n",
      "step 9: merging \"_\" and \"a\"\n",
      "step 10: merging \"_\" and \"p\"\n",
      "step 11: merging \"en\" and \"t\"\n",
      "step 12: merging \"e\" and \"d</w>\"\n",
      "step 13: merging \"_\" and \"w\"\n",
      "step 14: merging \",\" and \"</w>\"\n",
      "step 15: merging \"_\" and \"t\"\n",
      "step 16: merging \"n\" and \"</w>\"\n",
      "step 17: merging \"_p\" and \"a\"\n",
      "step 18: merging \"_pa\" and \"r\"\n",
      "step 19: merging \"_par\" and \"ent\"\n",
      "step 20: merging \"en\" and \"</w>\"\n",
      "The bpe tokens are: \n",
      "w: 0\n",
      "D: 1\n",
      "k: 2\n",
      "_: 3\n",
      "T: 4\n",
      "h: 5\n",
      "n: 6\n",
      "p: 7\n",
      "o: 8\n",
      "]: 9\n",
      "b: 10\n",
      "m: 11\n",
      "u: 12\n",
      "a: 13\n",
      "W: 14\n",
      "r: 15\n",
      "B: 16\n",
      "v: 17\n",
      "e: 18\n",
      "t: 19\n",
      "6: 20\n",
      "[: 21\n",
      "l: 22\n",
      "d: 23\n",
      "y: 24\n",
      ",: 25\n",
      "c: 26\n",
      "f: 27\n",
      "O: 28\n",
      "g: 29\n",
      "H: 30\n",
      "</w>: 31\n",
      "s: 32\n",
      "': 33\n",
      "i: 34\n",
      "er: 35\n",
      "s</w>: 36\n",
      "e</w>: 37\n",
      "en: 38\n",
      "d</w>: 39\n",
      "_h: 40\n",
      "her: 41\n",
      "_b: 42\n",
      "_a: 43\n",
      "_p: 44\n",
      "ent: 45\n",
      "ed</w>: 46\n",
      "_w: 47\n",
      ",</w>: 48\n",
      "_t: 49\n",
      "n</w>: 50\n",
      "_pa: 51\n",
      "_par: 52\n",
      "_parent: 53\n",
      "en</w>: 54\n",
      "The ids of the tokenized sequence are: \n",
      "[3, 16, 35, 11, 13, 6, 33, 32, 53, 32, 3, 23, 34, 17, 8, 15, 26, 18, 23, 47, 5, 38, 40, 18, 47, 13, 32, 3, 32, 18, 17, 54, 3, 4, 41, 18, 13, 27, 19, 35, 25, 40, 18, 3, 32, 7, 22, 34, 19, 49, 34, 11, 18, 42, 18, 19, 0, 18, 38, 3, 18, 13, 26, 5, 53, 33, 32, 40, 8, 12, 32, 18, 5, 8, 22, 23, 3, 12, 6, 19, 34, 22, 40, 18, 3, 45, 35, 18, 23, 3, 26, 8, 22, 22, 18, 29, 37, 3, 21, 20, 9, 3, 30, 34, 32, 3, 27, 13, 19, 41, 3, 15, 18, 22, 8, 26, 13, 19, 18, 23, 49, 8, 3, 1, 13, 22, 22, 13, 32, 3, 27, 8, 15, 43, 44, 8, 32, 34, 19, 34, 8, 6, 43, 32, 43, 3, 22, 8, 10, 10, 24, 34, 32, 19, 3, 8, 6, 42, 18, 5, 13, 22, 27, 3, 8, 27, 3, 27, 8, 8, 23, 32, 35, 17, 34, 26, 18, 42, 12, 32, 34, 6, 18, 32, 32, 18, 32, 25, 47, 5, 34, 22, 18, 40, 34, 32, 3, 11, 8, 19, 41, 3, 11, 8, 17, 18, 23, 42, 13, 26, 2, 3, 34, 6, 47, 34, 19, 5, 40, 35, 53, 32, 3, 34, 6, 3, 14, 8, 8, 32, 19, 35, 25, 3, 28, 5, 34, 8, 25, 43, 6, 23, 42, 18, 26, 13, 11, 18, 43, 49, 18, 13, 26, 41, 49, 41, 37]\n",
      "\n",
      "The sequence corresponding to ids is: \n",
      "_ B er m a n ' s _parent s _ d i v o r c e d _w h en _h e _w a s _ s e v en</w> _ T her e a f t er , _h e _ s p l i t _t i m e _b e t w e en _ e a c h _parent ' s _h o u s e h o l d _ u n t i l _h e _ ent er e d _ c o l l e g e</w> _ [ 6 ] _ H i s _ f a t her _ r e l o c a t e d _t o _ D a l l a s _ f o r _a _p o s i t i o n _a s _a _ l o b b y i s t _ o n _b e h a l f _ o f _ f o o d s er v i c e _b u s i n e s s e s , _w h i l e _h i s _ m o t her _ m o v e d _b a c k _ i n _w i t h _h er _parent s _ i n _ W o o s t er , _ O h i o , _a n d _b e c a m e _a _t e a c her _t her e</w>\n"
     ]
    }
   ],
   "source": [
    "# Sentence piece example usage: \n",
    "corpus = '''Berman's parents divorced when he was seven. \n",
    "Thereafter, he split time between each parent's household until he entered college.[6] \n",
    "His father relocated to Dallas for a position as a lobbyist on behalf of foodservice businesses, \n",
    "while his mother moved back in with her parents in Wooster, Ohio, and became a teacher there'''\n",
    "data = corpus.split('.') \n",
    "  \n",
    "n = 20 # number of merge operations\n",
    "id_to_tokens, token_to_ids = byte_pair_encoding(data, n, use_sentence_piece=True)\n",
    "\n",
    "for i, p in enumerate(data):\n",
    "    p = p.strip()\n",
    "    p = re.sub(r\"\\s+\", \" \", p)\n",
    "    data[i] = \"_\" + p.replace(\" \", \"_\")\n",
    "\n",
    "# print(f\"data is {data}\")\n",
    "\n",
    "token_ids = tokenize(data, token_to_ids)\n",
    "\n",
    "print(\"The bpe tokens are: \")\n",
    "for tk, tid in token_to_ids.items():\n",
    "    print(\"{}: {}\".format(tk, tid))\n",
    "\n",
    "print(\"The ids of the tokenized sequence are: \")\n",
    "print(token_ids)\n",
    "print()\n",
    "print(\"The sequence corresponding to ids is: \")\n",
    "print(' '.join(id_to_tokens[tid] for tid in token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
