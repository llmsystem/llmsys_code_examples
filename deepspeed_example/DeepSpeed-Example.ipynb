{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOm999wxkBzBdw34in6tbAt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This is adopted from https://github.com/deepspeedai/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/README.md.\n","\n","We only demonstrate the SFT step here, but feel free to try out the reward modeling and RLHF training!"],"metadata":{"id":"edL-x2OUcFsT"}},{"cell_type":"markdown","source":["Env Setup: Make sure you are using GPU runtime.(e.g. T4)"],"metadata":{"id":"a8fmdQUScgoE"}},{"cell_type":"markdown","source":["Inside the training script `main.py`, we will need some modifications...\n","\n","(Note: libraries such huggingface accelerate integrates with Deepspeed, see https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed for more details!)"],"metadata":{"id":"ztHx5i5ee1I9"}},{"cell_type":"code","source":["!pip install deepspeed"],"metadata":{"id":"noq0zvYqn7kC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import deepspeed\n","from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n","from deepspeed import get_accelerator\n","\n","import argparse\n","import torch\n","\n","#skip the details here, please check actual main.py\n","def get_train_ds_config(offload,\n","                        dtype,\n","                        stage=2,\n","                        enable_hybrid_engine=False,\n","                        inference_tp_size=1,\n","                        release_inference_cache=False,\n","                        pin_parameters=True,\n","                        tp_gather_partition_size=8,\n","                        max_out_tokens=512,\n","                        enable_tensorboard=False,\n","                        enable_mixed_precision_lora=False,\n","                        tb_path=\"\",\n","                        tb_name=\"\"):\n","    pass\n","def parse_args():\n","    pass\n","def to_device():\n","    pass\n","\n","def main():\n","    args = parse_args()\n","\n","    if args.local_rank == -1:\n","        device = torch.device(get_accelerator().device_name())\n","    else:\n","        get_accelerator().set_device(args.local_rank)\n","        device = torch.device(get_accelerator().device_name(), args.local_rank)\n","        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        # torch.distributed.init_process_group(backend='nccl')\n","        deepspeed.init_distributed()\n","    ds_config = get_train_ds_config(offload=args.offload,\n","                                    dtype=args.dtype,\n","                                    stage=args.zero_stage,\n","                                    enable_tensorboard=args.enable_tensorboard,\n","                                    tb_path=args.tensorboard_path,\n","                                    tb_name=\"step1_model\")\n","    # Deepspeed provide custom cpu adam if we want to offload optimizer to cpu\n","    AdamOptimizer = DeepSpeedCPUAdam if args.offload else FusedAdam\n","    # Initialize model with deepspeed\n","    # DeepSpeed model training is accomplished using the DeepSpeed engine.\n","    # The engine can wrap any arbitrary model of type torch.nn.module and\n","    # has a minimal set of APIs for training and checkpointing the model.\n","    model, optimizer, _, lr_scheduler = deepspeed.initialize(\n","        model=model,\n","        optimizer=optimizer,\n","        args=args,\n","        config=ds_config,\n","        lr_scheduler=lr_scheduler,\n","        dist_init_required=True)\n","    train_dataloader = None\n","    #DeepSpeed automatically performs the necessary operations required for distributed data parallel training,\n","    #in mixed precision, with a pre-defined learning rate scheduler. No code change needed.\n","    for epoch in range(args.num_train_epochs):\n","        model.train()\n","        import time\n","        for step, batch in enumerate(train_dataloader):\n","            start = time.time()\n","            batch = to_device(batch, device)\n","            outputs = model(**batch, use_cache=False)\n","            loss = outputs.loss\n","            if args.print_loss:\n","                print(\n","                    f\"Epoch: {epoch}, Step: {step}, Rank: {torch.distributed.get_rank()}, loss = {loss}\"\n","                )\n","            model.backward(loss)\n","            model.step()\n","            end = time.time()"],"metadata":{"id":"FVdLubB9eq1G","executionInfo":{"status":"ok","timestamp":1743549424621,"user_tz":240,"elapsed":28,"user":{"displayName":"Kath Choi","userId":"14493180204401828909"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Let's see what is needed to run the training script with deepspeed command line:\n","1. specify your training script `main.py`\n","2. specify args such as `model_name_or_path`, `zero_stage` etc.\n","\n","To see the full list of args supported, see https://www.deepspeed.ai/docs/config-json/"],"metadata":{"id":"-dlTlGeTeNxx"}},{"cell_type":"markdown","source":["Recall that:\n","1. Optimizer state partitioning (ZeRO stage 1)\n","2. Gradient partitioning (ZeRO stage 2)\n","3. Parameter partitioning (ZeRO stage 3)"],"metadata":{"id":"il2y5cowhv43"}},{"cell_type":"markdown","source":["The bash scipt looks something like this"],"metadata":{"id":"Swm3dKlOm4RJ"}},{"cell_type":"code","source":["\"\"\"\n","ZERO_STAGE=$1\n","OUTPUT=./output_llama2_7b\n","if [ \"$ZERO_STAGE\" == \"\" ]; then\n","    ZERO_STAGE=3\n","fi\n","mkdir -p $OUTPUT\n","\n","deepspeed main.py \\\n","   --data_split 2,4,4 \\\n","   --model_name_or_path meta-llama/Llama-2-7b-hf \\\n","   --per_device_train_batch_size 1 \\\n","   --per_device_eval_batch_size 4 \\\n","   --max_seq_len 512 \\\n","   --learning_rate 9.65e-6 \\\n","   --weight_decay 0. \\\n","   --num_train_epochs 3  \\\n","   --gradient_accumulation_steps 4 \\\n","   --lr_scheduler_type cosine \\\n","   --num_warmup_steps 0 \\\n","   --seed 1234 \\\n","   --gradient_checkpointing \\\n","   --dtype bf16 \\\n","   --zero_stage $ZERO_STAGE \\\n","   --deepspeed \\\n","   --output_dir $OUTPUT \\\n","\"\"\""],"metadata":{"id":"tlbF27LkeFBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we can run the command in a cluster and monitor the nvidia-smi to see the how much memory is saved!"],"metadata":{"id":"B8iioP1jSivH"}}]}